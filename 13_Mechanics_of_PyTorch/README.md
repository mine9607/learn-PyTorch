# The Mechanics of PyTorch

## The Key features of PyTorch

1. PyTorch uses dynamic computational graphs, which are more flexible and debugger friendly than static competitors. With PyTorch you can execute the code line by line having full access to all variables

2. PyTorch has the ability to work with single or multiple GPUs--allowing efficient training on large datasets and large-scale systems

3. PyTorch supports mobile development, making it suitable for production.

## PyTorch's Computation Graphs

`Directed acyclic graph (DAG)` - used to derive relationships between tensors from the input all the way to the output.

Computation graphs assign nodes for each variable (tensor) and for each result of a mathematical operation.

![Simple Illustration of Computation Graph](./comp-graph.png)

In the example calculation: $z = 2 \times (a-b) +c $. Each variable (a,b,c) is a node which holds the value of the tensor. Each tensor value resulting from a mathematical operation (PEMDAS) is stored in a node. The resulting graph shows the relationship between inputs and output of the computation.

## PyTorch tensor objects for storing and updating model parameters

A special tensor object for which gradients need to be computed allows us to store and update the parameters (**weights**) of our models during training. To create this tensor object simple add `requires_grad=True` to the user specified initial values.

> Note: only tensors of type `floating point` or `complex dtype` can require gradients

```python
a = torch.tensor(3.14, requires_grad=True)
print(a)
b = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
print(b)

# note requires_grad = False by default and can more efficiently be set to true by the .requires_grad_() method

w = torch.tensor([1.0, 2.0, 3.0])
print(w.requires_grad)

w.requires_grad_()
print(w.requires_grad)

```

> Note: it is also important when creating weight tensors to implement a random initialization scheme.

### Initialization Schemes

#### Glorot Initialization

Supports both glorot uniform and normal distribution of weights

`nn.init.xavier_normal_()` - initializes a weight tensor to values from random normal distribution

1. Create an empty tensor and an operator called `init` as an object of class `GlorotNormal`
2. Fill the tensor with values according to the Glorot Initialization by calling: `xavier_normal_()` method.

```python
torch.manual_seed(1)
w = torch.empty([2,3])
nn.init.xavier_normal_(w)
print(w)
```

This is implemented more realistically as follows:

```python

class MyModule(nn.Module):
    def __init__(self):
    super()__init__()
    w1 = torch.empty(2,3,requires_grad=True)
    nn.init.xavier_normal_(w1)
    w2 = torch.empty(1,2,requires_grad=True)
    nn.init.xavier_normal_(w2)
```

## Computing gradients via automatic differentiation

PyTorch supports backpropagation via `automatic differentiation` for model training via optimization algorithms such as `SGD`.

Automatic differentiation can be thought of as an implementation of the `chain rule` for computing gradients of nested fucntions.

To compute the gradients we call the `backward()` method provided by the `torch.autograd` module

```python
w = torch.tensor(1.0, requires_grad=True)
b = torch.tensor(0.5, requires_grad=True)
x = torch.tensor([1.4])
y = torch.tensor([2.1])
z = torch.add(torch.mul(w, x), b)
loss = (
    (y - z).pow(2).sum()
)  # loss function (partial derivative of sigmoid activation function)
loss.backward()
print("dL/dw: ", w.grad)
print("dL/db: ", b.grad)
```

> NOTE: Computing gradients of the loss w.r.t. the input example `(dL/dx)` is used for generating `adversarial examples` (aka `adversarial attack`). In computer vision, adversarial examples are examples that are generated by adding some small, imperceptible noise (perturbations) to the input example, which results in a deep NN misclassifying them. Remember that in traditional backprop we are calculating dL/dw and dL/db **NOT** dL/dx.

## Simplifying Implementations via torch.nn module

### nn.Sequential

With `nn.Sequential` module we build a feed-forward nn model where the layers stored inside the model are connected in a cascaded way.

```python
# Build a 2-layer fully-connected model using nn.Sequential
model = nn.Sequential(
    nn.Linear(4, 16),
    nn.ReLU(),
    nn.Linear(16, 32),
    nn.ReLU(),
)

print(model)

# configure the first layer by specifying the initial value distribution for the weight
nn.init.xavier_uniform(model[0].weight)

# configure the second layer by computing the l1 penalty term for the weight matrix:
l1_weight = 0.01
l1_penalty = l1_weight * model[2].weight.abs().sum()
```

### Choosing a Loss function

For optimization algorithms: `SGD` and `Adam` are the most commonly used methods.

The choice of loss function depends on the task:

- `MSE` for a regression problem
- `Cross-entropy` for classification tasks

> NOTE: Previous metrics are also applicable:`Precision`, `Recall`, `Accuracy`, `Area under Curve (AUC)`, `False Negative Score`, `False Positive Score`, `F1`

## Solving an XOR Classification Problem

## Making Model Building more flexible with nn.Module

The benefit of `nn.Module` over the `nn.Sequential` class is that it allows us to have more control and build more complex models with:

- multiple inputs
- multiple outputs
- intermediate branches

### Writing Custom layers in PyTorch

Used when you want to create a layer that isn't supported by PyTorch or customizing an existing layer

Example:

```python
class NoisyLiner(nn.Module):
    def __init__(self, input_size, output_size, noise_stddev=0.1):
        super().__init__()
        w = torch.Tensor(input_size, output_size)
        self.w = nn.Parameter(w) ## nn.Parameter is a Tensor that's a module parameter
        nn.nit.xavier_uniform_(self.w)
        b = torch.Tensor(output_size).fill_(0)
        self.b = nn.Parameter(b)
        self.noise_stddev = noise_stddev

    def forward(self, x, training=False):
        if training:
            noise = torch.normal(0.0, self.noise_stddev, x.shape)
            x_new = torch.add(x, noise)
        else:
            x_new = x
        return torch.add(torch.mm(x_new, self.w), self.b)

```
