# Parallelizing Neural Network Training with PyTorch

## PyTorch and training performance

### Performance challenges

### What is PyTorch

## First Steps with PyTorch:

### Installing PyTorch

### Creating `tensors` in PyTorch

### Manipulating data type and shape of a tensor

### Applying mathematical operations to tensors

### Split, stack, and concatenate tensors

## Building Input Pipelines in PyTorch:

### Creating a PyTorch DataLoader from existing tensors

### Combining two tensors into a joint dataset

### `Shuffle`, `batch`, and `repeat`

### Creating a dataset from files on local storage disk

### Fetching available datasets from the `torchvision.datasets` library

## Building a NN model in PyTorch

### The PyTorch NN module (`torch.nn`)

### Building a linear regression model

### Model training via the torch.nn and torch.optim modules

### Building a multilayer perceptron for classifying flowers in the Iris dataset

### Evaluating the trained model on the test dataset

### Saving and reloading the trained Model

## Choosing activation functions for multi-layer neural networks (MLNNs)

### Logistic function recap

### Estimating class probabilities in multiclass classification via the `softmax` function

### Broadening the output spectrum using a hyperbolic tangent

### Rectified Linear Unit (ReLU) activation
